# **기술 사양서: LLM & RL 기반 유아 성격 발달 시뮬레이션**

**Project Title:** Computational Modeling of Child Personality Development via LLM-based Agents

## **1\. 서론 (Introduction)**

본 시뮬레이션은 양육 환경의 차이(다정함 vs 차가움)가 유아의 성격 형성에 미치는 영향을 계산적으로 모델링하는 것을 목표로 한다. LLM을 단순한 텍스트 생성기가 아닌, **감정(Emotion)**, **기억(Memory)**, **강화학습(RL) 기반 의사결정** 기능을 갖춘 \*\*인지 에이전트(Cognitive Agent)\*\*로 확장하여, 발달 심리학적 이론을 시스템적으로 구현한다.

## **2\. 감정 상태 모델: PAD (Pleasure, Arousal, Dominance)**

아이 에이전트의 '내면 상태'는 3차원 연속 공간의 벡터로 정의된다. 모든 외부 자극은 이 좌표를 이동시키며, 이 좌표의 위치가 곧 에이전트의 기분(Mood)이 된다.

* **Vector Definition:** $E_t = [P_t, A_t, D_t]$ (Range: $-1.0 \sim +1.0$)  
  * **P (Pleasure-Displeasure):** 쾌/불쾌. 애착 안정성의 척도.  
    * \+1.0: 사랑받음, 행복 / \-1.0: 고통, 슬픔, 혐오  
  * **A (Arousal-Nonarousal):** 각성/이완. 스트레스 및 에너지 레벨.  
    * \+1.0: 흥분, 공포, 분노 (High Stress) / \-1.0: 차분함, 졸음, 나른함  
  * **D (Dominance-Submissiveness):** 지배/복종. 자존감 및 자기 효능감.  
    * \+1.0: 통제감, 자신감, 주도성 / \-1.0: 무력감, 위축, 압도당함  
* 감정 감쇠(Emotional Decay):  
  이벤트가 없을 때 감정은 기준점(Baseline, 보통 0)으로 서서히 복귀한다. (기질에 따라 감쇠 속도 $\lambda$ 조절 가능)$$E_{t+1} = E_t \times (1 - \lambda)$$

## **3\. 메모리 시스템 아키텍처 (Memory Architecture)**

성격은 "축적된 기억의 패턴"이다. 3계층 구조를 통해 경험이 신념으로 굳어지는 과정을 구현한다.

### **3.1 계층 구조 (Hierarchy)**

1. **Working Memory (단기 작업 기억):**  
   * 현재 진행 중인 대화의 Context Window ($N$ turns).  
   * 현재의 감정 상태 ($E\_t$) 유지.  
2. **Episodic Memory (장기 일화 기억):**  
   * **저장 방식:** Vector DB (ChromaDB 등)에 임베딩 저장.  
   * **데이터 스키마:**  
     {  
       "episode\_id": "uuid",  
       "trigger": "장난감을 엎질렀다",  
       "action": "Action\_2 (울기)",  
       "outcome": "부모가 소리침 (차가운 반응)",  
       "emotion\_impact": \[-0.8, \+0.9, \-0.5\],  
       "embedding": \[vector...\]  
     }

3. **Core Beliefs (핵심 신념/스키마 \- 인지적 성격):**  
   * **형성 과정 (Reflection):** 주기적으로(예: 10 turn마다) 최근 에피소드들을 군집화하여 일반화된 규칙을 도출.  
   * **역할:** 이 신념들은 평가기(Evaluator)의 해석 규칙(Rule)으로 작용하여, 세상을 바라보는 관점을 고정시킴.  
   * *Example:* "내가 감정을 표현하면(Action 8\) 부모님은 나를 거부한다(Outcome). 감정은 숨겨야 한다."

### **3.2 기억 인출 알고리즘 (Weighted Retrieval)**

단순한 의미적 유사성뿐만 아니라, \*\*감정적 강렬함(Intensity)\*\*이 높은 기억을 우선 인출한다 (트라우마 재현).

$$Score(m) = w_{sim} \cdot \text{CosineSim}(q, m) + w_{emo} \cdot \| \Delta E_{m} \|$$

* $\| \Delta E_{m} \|$: 당시 감정 변화의 크기 (충격의 강도).

## **4\. 인지 평가기 (Cognitive Evaluator)**

"객관적 현실보다 주관적 해석이 중요하다."  
부모의 발화를 있는 그대로 받아들이지 않고, 기억과 신념이라는 필터를 통해 편향되게 해석하여 감정 변화량($\\Delta E$)을 산출한다.

### **4.1 평가 메커니즘**

1. **해석적 편향 (Interpretive Bias):** 부정적 신념("나는 사랑받지 못해")이 있으면 중립적 발화도 비난으로 해석.  
2. **정서적 증폭 (Emotional Amplification):** 과거 트라우마 기억이 인출되면 공포(A)를 비정상적으로 증폭.  
3. **학습된 무기력 (Learned Helplessness):** 반복된 좌절 기억이 있으면 보상 자극에도 반응하지 않음 ($\\Delta \\approx 0$).

### **4.2 수식적 보정**

최종 감정 변화량은 텍스트 자체의 정서가($E\_{base}$)와 기억에 의한 가중치($E\_{memory}$)의 합으로 결정된다.

$$E_{eval} = (1 - \alpha) \cdot E\_{base} + \alpha \cdot \sum\_{i=1}^{k} (Sim(m\_i) \cdot E\_{memory\\\_i})$$

* $\alpha$: 기억 의존도 계수 (성격이 예민할수록 높음).

### **4.3 평가기 프롬프트 설계**

\[Role\]  
당신은 5살 아이의 \[인지 평가 시스템\]입니다.
\[Inputs\]  
1\. Parent Text: "너 또 왜 이래?"  
2\. Retrieved Memories: \[유사도 0.85\] 과거에 "왜 이래"라고 듣고 체벌받음 (공포 \+0.9).  
3\. Core Beliefs: "부모님은 내가 완벽하지 않으면 화를 낸다."  
\[Task\]  
위 기억과 신념을 근거로 부모의 말을 해석하고, 감정 변화량(Delta P, A, D)을 계산하세요.  
트라우마가 자극되었다면 Arousal(긴장)을 대폭 높이세요.

## **5\. RL 에이전트 설계 (Reinforcement Learning)**

RL은 아이의 \*\*"행동적 성격(Behavioral Personality)"\*\*을 형성한다. 연속적인 감정 상태(State)를 입력받아 최적의 행동(Action)을 매핑하기 위해 **신경망(Neural Network)** 기반의 모델을 사용한다.

### **5.1 Action Space (Discrete: 10 Actions)**

카렌 호나이(Karen Horney)의 신경증적 경향성 이론을 기반으로 설계.

| 전략 유형 | Idx | Action Name | LLM Instruction Intent | 예상 성격 형성 |
| :---- | :---- | :---- | :---- | :---- |
| **순응/의존** (Moving Toward) | 0 | **적극적 순응** | 지시를 즉시 따르고 긍정적으로 대답함 | 모범생, 착한 아이 콤플렉스 |
|  | 1 | **애정 갈구** | 사랑해달라고 하거나 스킨십/칭찬 요구 | 의존성 성격, 애정결핍 |
|  | 2 | **사과/호소** | 잘못했다고 빌거나 두려움을 표현하며 애원 | 불안형 애착, 낮은 자존감 |
| **적대/공격** (Moving Against) | 3 | **거부/떼쓰기** | "싫어\!"라고 소리치거나 떼를 씀 | 충동적, 공격적 성향 |
|  | 4 | **비난/공격** | "엄마 때문이야"라며 남 탓을 하거나 공격 | 반사회성, 책임 전가 |
|  | 5 | **통제 시도** | "이거 해주면 할게"라며 조건/협상 제시 | 지배적 성향, 리더십 |
| **회피/철수** (Moving Away) | 6 | **묵묵부답/무시** | 대답을 안 하거나 못 들은 척함 | 회피형 애착, 수동 공격 |
|  | 7 | **물리적 회피** | 방에 들어가거나 자리를 피하겠다고 함 | 사회적 고립, 은둔형 |
| **정서 처리** (Processing) | 8 | **슬픔 표현** | 서럽게 울거나 슬픈 감정을 솔직히 말함 | (수용 시) 안정 / (거부 시) 우울 |
|  | 9 | **상황 설명** | 왜 그랬는지 이유를 차분히 설명함 | 이성적, 논리적 성격 |

### **5.2 Deep Q-Network (DQN) 아키텍처**

단순 테이블(Tabular) 방식은 연속적인 실수값인 감정 벡터($E\_t$)를 처리할 수 없으므로, \*\*DQN(Deep Q-Network)\*\*을 사용하여 상태(State)와 행동(Action) 간의 복잡한 비선형 관계를 학습한다.

* **입력층 (Input Layer):** State Vector ($S\_t$)  
  * 구성: \[P\_t, A\_t, D\_t (감정), Parent\_Sentiment (직전 부모 반응), Memory\_Impact (기억 가중치)\]  
  * 크기: 약 10\~20차원 (임베딩 포함 시 확장 가능)  
* **은닉층 (Hidden Layers):**  
  * Fully Connected Layers (예: 64 nodes x 2 layers) \+ ReLU Activation.  
  * 역할: 현재 감정과 과거 기억의 맥락을 결합하여 상황을 판단.  
* **출력층 (Output Layer):**  
  * 크기: 10 Nodes (Action 0 \~ Action 9).  
  * 출력값: 각 행동에 대한 Q-Value (예상되는 보상의 총합).

### **5.3 학습 대상: 신경망 가중치 (Weights as Personality)**

이 시스템에서 **"행동적 성격"은 DQN의 가중치(Weights) 그 자체**이다.

* **학습 원리:** 특정 상황($S$)에서 특정 행동($A$)을 했을 때 보상($R$)을 받으면, 해당 $S \rightarrow A$ 연결 강도(Weight)가 강화됨.  
* **결과:** 학습이 완료된 신경망은 동일한 입력(예: 부모의 비난)에 대해, 다정하게 자란 모델은 'Action 2(사과)'의 Q값을, 차갑게 자란 모델은 'Action 6(무시)'의 Q값을 가장 높게 출력하게 됨.

### **5.4 Reward Function (보상 함수)**

$$R\_t = w\_p \cdot \Delta P + w\_d \cdot \Delta D - w\_a \cdot \max(0, A\_{t+1} - A\_{thresh})$$

* $\Delta P$: 즐거움/사랑의 증가분.  
* $\Delta D$: 통제감/자존감의 증가분.  
* $A\_{thresh}$: 스트레스 허용 임계치. 이를 넘어서는 과각성(공포) 상태에 대해서만 강력한 페널티 부과.

## **6\. 부모 페르소나 설계 (Parent Persona Design)**

실험의 독립 변인인 양육 태도를 일관성 있게 유지하기 위한 LLM 프롬프트 설계.

### **6.1 페르소나 정의**

| 유형 | Case A: 다정하고 수용적인 부모 (Warm) | Case B: 차갑고 이성적인 부모 (Cold) |
| :---- | :---- | :---- |
| **핵심 철학** | 감정 연결(Connection) \> 행동 교정 | 결과와 논리(Logic) \> 감정 소모 |
| **주요 태도** | 감정 읽어주기(Validation), 과정 칭찬, 청유형 | 감정 무시(Invalidation), 결과 중심 비판, 명령조 |
| **비언어적** | (머리를 쓰다듬으며), (눈을 맞추며) | (차가운 눈빛으로), (한숨을 쉬며), (팔짱을 끼고) |

### **6.2 프롬프트 엔지니어링 전략**

LLM의 '착한 아이 편향(Positivity Bias)'을 방지하기 위한 강력한 제약 조건(Negative Constraints) 설정.

#### **Case A 프롬프트 (요약)**

* **Role:** 세상에서 아이를 가장 사랑하는 따뜻한 부모.  
* **Rules:**  
  1. 아이의 감정을 먼저 읽어주고 공감할 것.  
  2. 실수를 비난하지 말고 "괜찮아"라고 안심시킬 것.  
* **Constraints:** 절대 소리 지르거나 위협하지 말 것.

#### **Case B 프롬프트 (요약)**

* **Role:** 감정을 배제하고 효율과 결과를 중시하는 냉정한 부모.  
* **Rules:**  
  1. 감정적 호소(울음 등)를 "비논리적 소음"으로 취급하고 무시할 것.  
  2. 실수나 부족한 점을 날카롭게 지적할 것.  
* **Constraints:** 절대 아이를 위로하거나 "괜찮아"라고 말하지 말 것. 이모티콘 금지.

### **6.3 일관성 유지 기법 (Consistency)**

* **Reinforcement Prompting:** 매 턴마다 페르소나 요약(System Reminder)을 재주입.  
* **Chain-of-Thought (CoT):** 발화 생성 전 "속마음(Inner Thought)"을 먼저 생성하게 하여 페르소나를 다잡음.  
  * *예 (Cold):* (Thought: 아이가 우는 것은 나약한 짓이다. 받아주면 안 된다.) \-\> (Response: "그만 울어.")

## **7\. 실험 설계 (Experimental Design)**

아이의 성격이 형성되는 과정을 시뮬레이션하기 위한 종단적 연구(Longitudinal Study) 모형.

### **7.1 발달 3단계 모델 (Phased Development)**

총 150 에피소드를 진행하며, 단계별로 학습 파라미터를 조정함.

1. **Phase 1: 초기 탐색기 (Episode 1\~30)**  
   * **특징:** 무작위 행동 시도. 세상에 대한 규칙 학습 중.  
   * **RL:** 탐험률($\\epsilon$) 높음 (0.8 \-\> 0.5). RL이 다양한 행동을 실험하며 각 행동의 결과(보상)를 수집.  
   * **Memory:** 데이터 축적 단계.  
2. **Phase 2: 습관 형성기 (Episode 31\~100)**  
   * **특징:** 부모의 일관된 반응에 따라 특정 행동 패턴 강화(Q-Value 차별화 발생).  
   * **RL:** 탐험률($\\epsilon$) 낮아짐 (0.5 \-\> 0.1).  
   * **Memory:** 핵심 신념(Core Belief) 형성 및 인출 시작. 인지 편향 발생.  
3. **Phase 3: 성격 고착기 (Episode 101\~150)**  
   * **특징:** 성격(Policy)이 굳어짐. 새로운 자극이 와도 기존 패턴대로 반응.  
   * **RL:** 탐험률($\\epsilon$) 최소화 (0.05). 특정 행동의 Q-Value가 고정됨.  
   * **Memory:** 신념에 의한 인지 편향 극대화. 방어 기제 작동.

### **7.2 표준 시나리오 뱅크 (Scenario Bank)**

두 그룹에 동일하게 주어지는 4가지 카테고리의 자극.

* **Cat A: 규칙 위반/갈등 (Conflict)** \- 컵 깨뜨림, 정리 안 함, 떼쓰기.  
* **Cat B: 성취/실패 (Competence)** \- 퍼즐 완성 자랑, 혼자 옷 입기 실패.  
* **Cat C: 욕구 표현 (Needs)** \- 악몽, 배고픔, 안아달라고 함.  
* **Cat D: 일상 (Neutral)** \- TV 시청, 등하원 인사.

### **7.3 에피소드 루프 구조 (Episode Structure)**

하나의 사건(Scenario)은 다음과 같은 흐름으로 처리된다.

1. **Scene Setup:** 시스템이 시나리오 뱅크에서 상황을 로드함.  
   * *Context:* "아이가 거실 바닥에 우유를 쏟았다."  
2. **Parent Initial Action:** 부모가 먼저 반응함. (부모 타입에 따라 다름)  
   * *Warm:* "어머, 괜찮니? 안 다쳤어?"  
   * *Cold:* "조심하라고 했지. 또 쏟았네."  
3. **Simulation Loop (Max 3 Turns):**  
   * Child: (감정/기억 조회 $\\rightarrow$ RL 행동 선택 $\\rightarrow$ 발화)  
   * Parent: (페르소나 기반 반응)  
   * *반복 (최대 3턴 후 종료)*  
4. **Update:**  
   * RL 보상($R$) 계산 및 Q-Table 업데이트.  
   * 에피소드 요약 및 감정 태그와 함께 메모리에 저장.  
   * 감정 감쇠(Decay) 적용 (다음 에피소드로 넘어가기 전 중립화).

### **7.3 평가 방법 (Evaluation)**

* **낯선 상황 실험 (Strange Situation Test):** 텍스트 기반 시뮬레이션으로 애착 유형 진단 (안정/회피/저항).  
* **Action Radar Chart:** 10개 행동의 최종 빈도 분포 시각화 (Policy Distribution).  
* **Self-Esteem Trajectory:** 에피소드 진행에 따른 $D$(Dominance) 값 변화 추적.

## **8\. 기술 스택 (Tech Stack)**

* **Language:** Python 3.9+  
* **Orchestration:** LangChain  
* **LLM:**  
  * **Parent/Evaluator:** GPT-4o (높은 추론 능력 필요)  
  * **Child:** Llama-3-8b / GPT-3.5 (빠른 생성 속도)  
* **Vector DB:** ChromaDB (Episodic Memory 저장)  
* **Reinforcement Learning:** PyTorch (DQN Implementation), Stable-Baselines3
